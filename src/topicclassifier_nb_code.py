# -*- coding: utf-8 -*-
"""Topicclassifier_NB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bCkV0yJF1XEHFMDAlGDo-qXKwl3hfs7M
"""

import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import os
import string
from nltk.corpus import stopwords

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')

import pandas as pd

# Mount Google Drive to access files (if file is stored in Google Drive)
from google.colab import drive
drive.mount('/content/drive')

# Provide the path to your Excel file
file_path = '/content/drive/My Drive/merged_scrap_excel.xlsx'  # Update with your file path

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Load your dataset containing 50,000 records
full_data = pd.read_excel(file_path)

# Randomly sample 10,000 records
data = full_data.sample(n=20000, random_state=42)  # Change 'random_state' for different samples
# sampled_data = full_data

# Combine 'Title' and 'Summary' columns
data['Text'] = data['Title'] + ' ' + data['Summary']

# Text preprocessing function
def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercasing and removing punctuation
    tokens = [word.lower() for word in tokens if word.isalpha()]
    tokens = [word for word in tokens if word not in string.punctuation]

    # Removing stop words
    # Stop words removal
    custom_stopwords_file = '/content/drive/My Drive/stopwords-en.txt'
    # Read custom stopwords from file
    custom_stopwords = set()
    if os.path.exists(custom_stopwords_file):
        with open(custom_stopwords_file, 'r',encoding='utf-8') as file:
            custom_stopwords = set(file.read().splitlines())

    # Add NLTK's default English stopwords to your custom stopwords
    stop_words = set(stopwords.words('english'))
    stop_words.update(custom_stopwords)
    filtered_tokens = [token for token in tokens if token not in stop_words]
        # stop_words = set(stopwords.words('english'))
        # tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]

    # Join tokens back to text
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Preprocess the text data
data['Preprocessed_Text'] = data['Text'].apply(preprocess_text)

X = data['Preprocessed_Text']
y = data['Main Topic']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Rest of your machine learning workflow (preprocessing, modeling, training, testing) goes here...
# Feature extraction using TF-IDF
vectorizer = CountVectorizer(analyzer= 'word')
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Train a text classification model (Naive Bayes as an example)
classifier = MultinomialNB()
classifier.fit(X_train_vectorized, y_train)

# Predict using the trained model
predictions = classifier.predict(X_test_vectorized)

# Evaluate the classifier
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy}')

# Classification report
print(classification_report(y_test, predictions))

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
#report=classification_report(y_test, predictions))

# Sample classification report (replace this with your actual classification report)
report = '''
               precision    recall  f1-score   support

        Economy       0.62      0.67      0.69       413
      Education       0.60      0.63      0.62       410
  Entertainment       0.70      0.81      0.75       401
    Environment       0.76      0.75      0.80       390
           Food       0.77      0.62      0.68       386
         Health       0.79      0.77      0.78       404
       Politics       0.74      0.85      0.83       404
         Sports       0.80      0.65      0.74       408
     Technology       0.71      0.76      0.73       403
         Travel       0.71      0.63      0.67       381
'''

# Parsing the classification report and extracting metrics
report = report.split('\n')
classes = []
metrics = []

for line in report[2:-1]:  # Skip header and footer lines
    line = line.split()
    if len(line) == 0:
        continue
    classes.append(line[0])
    metrics.append([float(x) for x in line[1:]])

# Extracting metrics for precision, recall, and f1-score
precision = [metric[0] for metric in metrics]
recall = [metric[1] for metric in metrics]
f1_score = [metric[2] for metric in metrics]

# Plotting the bar graphs
fig, ax = plt.subplots(figsize=(10, 6))

bar_width = 0.25
index = range(len(classes))

bar1 = plt.bar(index, precision, bar_width, label='Precision')
bar2 = plt.bar([i + bar_width for i in index], recall, bar_width, label='Recall')
bar3 = plt.bar([i + 2 * bar_width for i in index], f1_score, bar_width, label='F1-Score')

plt.xlabel('Classes')
plt.ylabel('Scores')
plt.title('Classification Report Metrics')
plt.xticks([i + bar_width for i in index], classes)
plt.legend()

plt.tight_layout()
plt.show()

import string
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Lowercasing and removing punctuation
    tokens = [word.lower() for word in tokens if word.isalpha()]
    tokens = [word for word in tokens if word not in string.punctuation]

    # Removing stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    # Join tokens back to text
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text
input_text = "who is the president of usa"
cleaned_text = preprocess_text(input_text)

# vectorizer = CountVectorizer(analyzer= 'word')
# vec_clean_text =vectorizer.transform([cleaned_text])
# vec_clean_text

classifier.predict(vec_clean_text)

!pip install joblib

!pip install sklearn

# # Save the trained model to a file
# from joblib import dump
# model_filename = 'topic_classifier_NB.joblib'
# dump(classifier, model_filename)

from joblib import dump


# Assuming 'classifier' is your trained MultinomialNB() model
# and 'vectorizer' is your CountVectorizer() object

# Create a dictionary containing both the classifier and vectorizer
model_data = {
    'classifier': classifier,
    'vectorizer': vectorizer
}

# Save the dictionary containing both objects to a file
model_filename = 'topic_classifier_MLNB.joblib'
dump(model_data, model_filename)

from joblib import load
loaded_model = load('/content/drive/My Drive/topic_classifier_MLNB.joblib')

# Retrieve the classifier and vectorizer from the loaded model data
loaded_classifier = loaded_model['classifier']
loaded_vectorizer = loaded_model['vectorizer']

vectorized_text =loaded_vectorizer.transform([cleaned_text])

final_output=loaded_classifier.predict(vectorized_text)
final_output

type(list(final_output))

k= + list(final_output)